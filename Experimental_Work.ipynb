{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Experimental_Work.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNwpu1pf/4Ev+t7pnR8SQBT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raamav/ML_Projects_1/blob/master/Experimental_Work.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmNzwjaO0MFD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "2ced12e6-92ea-4e14-a9c1-58863af6d085"
      },
      "source": [
        "\"\"\"\n",
        "Purpose: Build a function that takes a list of sentences and generates a vocabulary dictionary off it\n",
        "Inputs: list of sentences\n",
        "Returns: a 'vocab' dictionary of comprising words and their frequencies\n",
        "\n",
        "\"\"\"\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nPurpose: Build a function that takes a list of sentences and generates a vocabulary dictionary off it\\nInputs: list of sentences\\nReturns: a 'vocab' dictionary of comprising words and their frequencies\\n\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9aMqq691-Cn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tU12ziK12dJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv(\"https://raw.githubusercontent.com/raamav/Text-Classification/master/train.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgpJczJw17kf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = data[['text']].copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVr8m0U22LyZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "c0e2f485-5109-4e3b-8ab4-454c6815fd3c"
      },
      "source": [
        "data.info()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 7613 entries, 0 to 7612\n",
            "Data columns (total 1 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   text    7613 non-null   object\n",
            "dtypes: object(1)\n",
            "memory usage: 59.6+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlvBoSGP2NbH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "dfe6ffe4-8c5f-46e0-e533-f71ef1ff301d"
      },
      "source": [
        "data.head(3)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text\n",
              "0  Our Deeds are the Reason of this #earthquake M...\n",
              "1             Forest fire near La Ronge Sask. Canada\n",
              "2  All residents asked to 'shelter in place' are ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIvdJ7aE3jHh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a text processing function\n",
        "\n",
        "import re\n",
        "import string\n",
        "\n",
        "def prepro(text):\n",
        "\n",
        "  \"\"\"\n",
        "  A text preprocessing function that converts to lower and gets rid of numbers and special characters\n",
        "  \"\"\"\n",
        "  text = text.lower()\n",
        "\n",
        "  text = re.sub(r\"[^a-z0-9 ]\",\"\",text)\n",
        "\n",
        "  return text\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwpPPsRJ3jMs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "e4d34ba7-805f-40cc-a903-526ba3fe3ef1"
      },
      "source": [
        "s1 = \"Differently than everyone else did using regex, I would try to exclude every character that is not what I want, instead of enumerating explicitly what I don't want.\"\n",
        "print(s1)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Differently than everyone else did using regex, I would try to exclude every character that is not what I want, instead of enumerating explicitly what I don't want.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QXOoX2B3jTC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "ed965e2e-705a-41d1-cf3d-6cacca8c8e85"
      },
      "source": [
        "s2 = prepro(s1)\n",
        "\n",
        "s3 = s2.split()\n",
        "\n",
        "print(s3)\n",
        "\n",
        "print(type(s3))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['differently', 'than', 'everyone', 'else', 'did', 'using', 'regex', 'i', 'would', 'try', 'to', 'exclude', 'every', 'character', 'that', 'is', 'not', 'what', 'i', 'want', 'instead', 'of', 'enumerating', 'explicitly', 'what', 'i', 'dont', 'want']\n",
            "<class 'list'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7VLHXsj2QnN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "8093152b-4244-411c-cad5-94795acde0bc"
      },
      "source": [
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "\n",
        "\n",
        "data['text'] = data.progress_apply(lambda x : prepro(x['text']), axis = 1 )\n",
        "\n",
        "print(data['text'][2])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 7613/7613 [00:00<00:00, 52848.96it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "all residents asked to shelter in place are being notified by officers no other evacuation or shelter in place orders are expected\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVokt9wX-Bfm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "efaa56e9-4150-42ad-af91-b3ce55aecf28"
      },
      "source": [
        "# printing out the first 5 examples\n",
        "\n",
        "for i in range(5):\n",
        "  print(data['text'].iloc[i])\n",
        "  print(\"\\n\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "our deeds are the reason of this earthquake may allah forgive us all\n",
            "\n",
            "\n",
            "forest fire near la ronge sask canada\n",
            "\n",
            "\n",
            "all residents asked to shelter in place are being notified by officers no other evacuation or shelter in place orders are expected\n",
            "\n",
            "\n",
            "13000 people receive wildfires evacuation orders in california \n",
            "\n",
            "\n",
            "just got sent this photo from ruby alaska as smoke from wildfires pours into a school \n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jr3-pyWo2knU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "13db2327-d80f-4cc7-914b-812f2ac60c6f"
      },
      "source": [
        ""
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7613"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlI_FI9A2pCV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ddd7f086-3f46-446c-e0f1-b7fd35a85b27"
      },
      "source": [
        "# convert the field to a list of sentences\n",
        "\n",
        "sentences = []\n",
        "m = len(data)\n",
        "\n",
        "for i in range(m):\n",
        "  sentences.append(data['text'].iloc[i])\n",
        "\n",
        "print(\"Number of sentences : \", len(sentences))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of sentences :  7613\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PYrmcSPGK7X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1338795b-766b-49ac-cec2-3e1c6451d9c1"
      },
      "source": [
        "type(sentences)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyp9LO1e2yIj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "9f105864-e536-463a-e068-03d91a10a490"
      },
      "source": [
        "# printing every 10th sentence\n",
        "\n",
        "for i in range(0,50,10):\n",
        "  print(\"\\n\",sentences[i])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " our deeds are the reason of this earthquake may allah forgive us all\n",
            "\n",
            " three people died from the heat wave so far\n",
            "\n",
            " this is ridiculous\n",
            "\n",
            " the end\n",
            "\n",
            " check these out httptcoroi2nsmejj httptco3tj8zjin21 httptcoyduixefipe httptcolxtjc87kls nsfw\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7Jmib9_-4Hr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# building a function that converts the list of sentences to a dictionary\n",
        "\n",
        "def build_vocab(sentences):\n",
        "  \n",
        "  \"\"\"\n",
        "  Converts a list of sentences into a vocabulary dictionary\n",
        "  Input, a list of sentences\n",
        "  Returns, a vocabulary dictionary {word: frequency}\n",
        "  \"\"\"\n",
        "\n",
        "  length = len(sentences)\n",
        "  vocab ={}\n",
        "\n",
        "  for s in sentences:\n",
        "    temp_list = []\n",
        "    temp_list = s.split()\n",
        "\n",
        "    for word in temp_list:\n",
        "      if word not in vocab.keys():\n",
        "        vocab[word] = 1\n",
        "      else:\n",
        "        vocab[word] += 1\n",
        "\n",
        "  return vocab\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-EAFMekFS1Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_dict = build_vocab(sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4g8SkvCFkKG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dc3ff6e3-4c02-4af2-e295-58af99e70515"
      },
      "source": [
        "print(\"length of the vocab list\", len(vocab_dict))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "length of the vocab list 22546\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zjy10AtGk3s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "2057bb09-e143-4636-d00e-ef46c84dd4ec"
      },
      "source": [
        "# TOP 50 WORDS\n",
        "print(\"\\n\")\n",
        "print(sorted(vocab_dict, key = lambda x: (-vocab_dict[x], x))[0:50])"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "['the', 'a', 'in', 'to', 'of', 'and', 'i', 'is', 'for', 'on', 'you', 'my', 'with', 'it', 'that', 'at', 'by', 'this', 'from', 'be', 'are', 'have', 'was', 'like', 'as', 'up', 'just', 'so', 'me', 'but', 'im', 'amp', 'not', 'your', 'its', 'out', 'after', 'will', 'all', 'when', 'an', 'no', 'fire', 'has', 'if', 'we', 'get', 'new', 'via', 'now']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKP6ymj_H6vO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "89296150-9ba7-4c3f-c529-7b57e68cc799"
      },
      "source": [
        "# TESTING THE DICTIONARY\n",
        "\n",
        "s1 = \"mary had a little lamb\"\n",
        "s2 = \"jack had a largist pony\"\n",
        "s3 = 'i am rama rama rama 1300'\n",
        "s4 = \"1300 le s lee lew i s s\"\n",
        "\n",
        "s5 = []\n",
        "\n",
        "s5 = [s1 , s2 , s3 , s4]\n",
        "\n",
        "print(s5)\n",
        "\n",
        "voc_dict = build_vocab(s5)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['mary had a little lamb', 'jack had a largist pony', 'i am rama rama rama 1300', '1300 le s lee lew i s s']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFk9dhxpIg5t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "78b0cd8a-b4c8-4bd2-9663-477312856adc"
      },
      "source": [
        "voc_dict"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'1300': 2,\n",
              " 'a': 2,\n",
              " 'am': 1,\n",
              " 'had': 2,\n",
              " 'i': 2,\n",
              " 'jack': 1,\n",
              " 'lamb': 1,\n",
              " 'largist': 1,\n",
              " 'le': 1,\n",
              " 'lee': 1,\n",
              " 'lew': 1,\n",
              " 'little': 1,\n",
              " 'mary': 1,\n",
              " 'pony': 1,\n",
              " 'rama': 3,\n",
              " 's': 3}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCNkIVv1IjcG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bb6d9ad4-2eae-4df3-d348-6542b112d724"
      },
      "source": [
        "print(sorted(voc_dict, key = lambda x: (-voc_dict[x], x))[0:4])"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['rama', 's', '1300', 'a']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLRO8emtJqxw",
        "colab_type": "text"
      },
      "source": [
        " ### Using Pre-Trained Embeddings\n",
        "\n",
        " Two Purposes:\n",
        " 1. Standardization of vocabulary\n",
        " 2. Creating Embedding Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSqlLTI8KBsS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "b3ce2727-0240-459a-efd6-525277f66b01"
      },
      "source": [
        "# IMPORTING THE EMBEDDINGS\n",
        "# THANKS TO: LAURENCE MORONEY, AI ADVOCATE AT GOOGLE \n",
        "\n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/glove.6B.100d.txt \\\n",
        "    -O /tmp/glove.6B.100d.txt\n",
        "\n",
        "embeddings_index = {};\n",
        "\n",
        "with open('/tmp/glove.6B.100d.txt') as f:\n",
        "    for line in f:\n",
        "        values = line.split();\n",
        "        word = values[0];\n",
        "        coefs = np.asarray(values[1:], dtype='float32');\n",
        "        embeddings_index[word] = coefs;"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-27 17:55:51--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/glove.6B.100d.txt\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 64.233.170.128, 2607:f8b0:400c:c07::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|64.233.170.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 347116733 (331M) [text/plain]\n",
            "Saving to: ‘/tmp/glove.6B.100d.txt’\n",
            "\n",
            "/tmp/glove.6B.100d. 100%[===================>] 331.04M   131MB/s    in 2.5s    \n",
            "\n",
            "2020-04-27 17:55:54 (131 MB/s) - ‘/tmp/glove.6B.100d.txt’ saved [347116733/347116733]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lSZY6wTKByo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f5d4444a-5bb6-4647-d0a2-cc49d53f1d99"
      },
      "source": [
        "print(\"Glove vocabulary size: \",len(embeddings_index))\n",
        "print(\"Glove embedding dimensions: \",len(embeddings_index['the']))"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Glove vocabulary size:  400000\n",
            "Glove embedding dimensions:  100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhOq_T2uKCVZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# WRITING A FUNCTION TO DETERMINE THE OVERLAP BETWEEN THE 2 DICTIONARIES\n",
        "\n",
        "import operator\n",
        "\n",
        "def check_overlap(vocab,embeddings):\n",
        "  \n",
        "  not_in_glove = {}\n",
        "  total_corpus_words = 0\n",
        "  total_oov_words = 0\n",
        "\n",
        "  for i in vocab.keys():\n",
        "    total_corpus_words += vocab[i] \n",
        "\n",
        "    if i not in embeddings.keys():\n",
        "      not_in_glove[i] = vocab[i]\n",
        "      total_oov_words += vocab[i]\n",
        "\n",
        "  x = len(not_in_glove)\n",
        "  y = len(vocab)\n",
        "  z = len(embeddings)\n",
        "\n",
        "\n",
        "  print(\"The vocabulary size is (unique word-count) : \",y)\n",
        "  print(\"The corpus size is (total, non-unique word-count) : \",total_corpus_words)\n",
        "  print(f\"Embeddings found for {y-x} ({round((y-x)*100/y,2)}) % of the words\")\n",
        "  print(f\"{round(total_oov_words*100/total_corpus_words,2)} % of the corpus vocabulary is unmapped\")\n",
        "\n",
        "  print(\"\\n Top 20 words by frequency that are not present in embeddings dictionary :\")\n",
        "  p = min(len(not_in_glove), 20)\n",
        "  print(sorted(not_in_glove, key = lambda x: (-not_in_glove[x], x))[0:p])\n",
        "\n",
        "  not_in_glove_sorted = sorted(not_in_glove.items(), key=operator.itemgetter(1))[::-1]\n",
        "  \n",
        "\n",
        "  return not_in_glove_sorted\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gt5SFy8VKCZE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "c8b2f535-73d2-4f3e-f81b-5ed36215d99e"
      },
      "source": [
        "check_overlap(voc_dict, embeddings_index)"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The vocabulary size is (unique word-count) :  16\n",
            "The corpus size is (total, non-unique word-count) :  24\n",
            "Embeddings found for 15 (93.75) % of the words\n",
            "4.17 % of the corpus vocabulary is unmapped\n",
            "\n",
            " Top 20 words by frequency that are not present in embeddings dictionary :\n",
            "['largist']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('largist', 1)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyivZl2rKCcf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "5b1ffe9c-4600-438e-8a59-a50f4a317082"
      },
      "source": [
        "oov = check_overlap(vocab_dict,embeddings_index)"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The vocabulary size is (unique word-count) :  22546\n",
            "The corpus size is (total, non-unique word-count) :  110754\n",
            "Embeddings found for 12341 (54.74) % of the words\n",
            "10.61 % of the corpus vocabulary is unmapped\n",
            "\n",
            " Top 20 words by frequency that are not present in embeddings dictionary :\n",
            "['mh370', 'prebreak', '16yr', 'nowplaying', 'typhoondevastated', 'bestnaijamade', '11yearold', 'gbbo', 'disea', 'reddits', 'funtenna', 'subreddits', 'lmao', 'youve', 'gtgt', 'utc20150805', 'sensorsenso', 'arianagrande', 'selfimage', 'sismo']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ti1p07j4KBv-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "12d6aded-1c72-4f03-cbdc-82bc691c4214"
      },
      "source": [
        "oov[0:20]"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('mh370', 71),\n",
              " ('prebreak', 30),\n",
              " ('16yr', 28),\n",
              " ('nowplaying', 26),\n",
              " ('typhoondevastated', 25),\n",
              " ('bestnaijamade', 24),\n",
              " ('11yearold', 21),\n",
              " ('gbbo', 20),\n",
              " ('disea', 19),\n",
              " ('reddits', 18),\n",
              " ('subreddits', 17),\n",
              " ('funtenna', 17),\n",
              " ('lmao', 16),\n",
              " ('youve', 14),\n",
              " ('utc20150805', 13),\n",
              " ('gtgt', 13),\n",
              " ('sensorsenso', 12),\n",
              " ('selfimage', 11),\n",
              " ('arianagrande', 11),\n",
              " ('yyc', 10)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0E9huVoa9Ilh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# FUNCTION TO CORRECT COMMON MIS-SPELLINGS\n",
        "\n",
        "# there is no need to do this"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}